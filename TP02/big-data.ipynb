{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# تثبيت مكتبة Kaggle\n!pip install kaggle\n\nimport os\n\n# قم بتعيين متغيرات البيئة باستخدام الأسرار التي أضفتها\nos.environ['KAGGLE_USERNAME'] = 'YOUR_KAGGLE_USERNAME' # استبدل باسم المستخدم الخاص بك\nos.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_KEY' # استبدل بمفتاح الـ API الخاص بك\n\n# ابحث عن مجموعة بيانات كبيرة وحملها (كمثال)\n# لاستخدام مجموعة بيانات أخرى، غيّر الأمر التالي\n!kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store\n\n# فك ضغط الملف\n!unzip ecommerce-behavior-data-from-multi-category-store.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T19:10:01.426483Z","iopub.execute_input":"2025-10-16T19:10:01.427652Z","iopub.status.idle":"2025-10-16T19:13:16.118600Z","shell.execute_reply.started":"2025-10-16T19:10:01.427618Z","shell.execute_reply":"2025-10-16T19:13:16.117520Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\nRequirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\nRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.8.3)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.3)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.20.3)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.5)\nRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\nRequirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\nRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\nDataset URL: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\nLicense(s): copyright-authors\nDownloading ecommerce-behavior-data-from-multi-category-store.zip to /kaggle/working\n 99%|██████████████████████████████████████▊| 4.27G/4.29G [00:05<00:00, 409MB/s]\n100%|███████████████████████████████████████| 4.29G/4.29G [00:05<00:00, 849MB/s]\nArchive:  ecommerce-behavior-data-from-multi-category-store.zip\n  inflating: 2019-Nov.csv            \n  inflating: 2019-Oct.csv            \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport time\n\n# تحديد اسم الملف\nfile_name = '2019-Oct.csv'\n\n# قياس الوقت والمساحة\nstart_time = time.time()\n\n# إنشاء مكرر (iterator) لقراءة الملف في أجزاء\nchunk_iter = pd.read_csv(file_name, chunksize=100000) # قراءة 100 ألف صف في كل مرة\n\n# معالجة كل جزء على حدة\n# في هذا المثال، سنقوم فقط بحساب عدد الصفوف\ntotal_rows = 0\nfor chunk in chunk_iter:\n    total_rows += len(chunk)\n    # يمكنك إجراء عمليات أخرى هنا على كل جزء\n    # مثل: chunk['price'].mean()\n\nend_time = time.time()\n\nprint(f\"Total rows: {total_rows}\")\nprint(f\"Time taken with chunks: {end_time - start_time:.2f} seconds\")\n\n# ملاحظة: استهلاك الذاكرة هنا يكون فقط لحجم الجزء الواحد، وليس الملف كاملاً.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T19:20:01.043912Z","iopub.execute_input":"2025-10-16T19:20:01.044239Z","iopub.status.idle":"2025-10-16T19:21:27.585183Z","shell.execute_reply.started":"2025-10-16T19:20:01.044213Z","shell.execute_reply":"2025-10-16T19:21:27.584192Z"}},"outputs":[{"name":"stdout","text":"Total rows: 42448764\nTime taken with chunks: 86.53 seconds\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import dask.dataframe as dd\nimport time\n\n# تحديد اسم الملف\nfile_name = '2019-Oct.csv'\n\n# قياس الوقت\nstart_time = time.time()\n\n# قراءة الملف باستخدام Dask (القراءة هنا كسولة \"lazy\"، أي لا يتم تحميل البيانات فعلياً)\ndask_df = dd.read_csv(file_name)\n\n# لتنفيذ عملية حسابية، يجب استدعاء compute()\n# هنا سنحسب عدد الصفوف\ntotal_rows = len(dask_df)\n\nend_time = time.time()\n\nprint(f\"Total rows: {total_rows}\")\nprint(f\"Time taken with Dask: {end_time - start_time:.2f} seconds\")\n\n# ملاحظة: Dask لا تقوم بتحميل البيانات إلا عند تنفيذ أمر يتطلب نتيجة مثل .compute() أو len()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T19:21:45.241297Z","iopub.execute_input":"2025-10-16T19:21:45.242009Z","iopub.status.idle":"2025-10-16T19:22:47.132055Z","shell.execute_reply.started":"2025-10-16T19:21:45.241980Z","shell.execute_reply":"2025-10-16T19:22:47.130994Z"}},"outputs":[{"name":"stdout","text":"Total rows: 42448764\nTime taken with Dask: 58.10 seconds\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\n\n# تحديد مسارات الملفات التي تريد حذفها\nzip_file_path = '/kaggle/working/ecommerce-behavior-data-from-multi-category-store.zip'\noct_csv_path = '/kaggle/working/2019-Oct.csv'\n\n# حذف الملفات لتفريغ مساحة على القرص\nif os.path.exists(zip_file_path):\n    os.remove(zip_file_path)\n    print(f\"تم حذف: {zip_file_path}\")\n\nif os.path.exists(oct_csv_path):\n    os.remove(oct_csv_path)\n    print(f\"تم حذف: {oct_csv_path}\")\n\nprint(\"\\nاكتمل التنظيف. لديك الآن مساحة أكبر على القرص. ✅\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T19:36:46.302100Z","iopub.execute_input":"2025-10-16T19:36:46.303468Z","iopub.status.idle":"2025-10-16T19:36:46.498051Z","shell.execute_reply.started":"2025-10-16T19:36:46.303417Z","shell.execute_reply":"2025-10-16T19:36:46.497013Z"}},"outputs":[{"name":"stdout","text":"تم حذف: /kaggle/working/ecommerce-behavior-data-from-multi-category-store.zip\nتم حذف: /kaggle/working/2019-Oct.csv\n\nاكتمل التنظيف. لديك الآن مساحة أكبر على القرص. ✅\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ----------------------------------------------------\n# الطريقة الثالثة: التحويل إلى Parquet للقراءة السريعة (النسخة المصححة)\n# ----------------------------------------------------\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport time # Make sure time is imported\n\nparquet_path = '/kaggle/working/data.parquet'\nfile_path = '/kaggle/working/2019-Nov.csv'\n\n# --- 1. عملية التحويل (تُنفذ مرة واحدة فقط) ---\nprint(f\"\\nبدء تحويل الملف {file_path} إلى صيغة Parquet...\")\nstart_time = time.time()\n\n# نستخدم chunksize لقراءة ملف CSV الضخم\nchunk_iterator = pd.read_csv(file_path, chunksize=500_000)\n\n# نأخذ أول قطعة لتحديد الهيكل (schema) للملف\nfirst_chunk = next(chunk_iterator)\nschema = pa.Table.from_pandas(first_chunk).schema\n\n# نفتح ملف Parquet للكتابة باستخدام ParquetWriter\n# NOTICE the corrected indentation below\nwith pq.ParquetWriter(parquet_path, schema=schema, compression='snappy') as writer:\n    # نكتب القطعة الأولى التي قرأناها بالفعل\n    writer.write_table(pa.Table.from_pandas(first_chunk))\n    \n    # نكمل كتابة باقي القطع في الملف\n    for chunk in chunk_iterator:\n        writer.write_table(pa.Table.from_pandas(chunk))\n\nend_time = time.time()\nprint(f\"اكتمل التحويل إلى Parquet في {end_time - start_time:.2f} ثانية.\")\n\n\n# --- 2. قراءة ملف Parquet السريعة (هذا الجزء يبقى كما هو) ---\nprint(\"\\nالآن، قراءة ملف Parquet المحول...\")\nstart_time_read = time.time()\n\ndf_from_parquet = pd.read_parquet(parquet_path)\n\nend_time_read = time.time()\nprint(f\"تمت قراءة ملف Parquet بالكامل في {end_time_read - start_time_read:.2f} ثانية فقط!\")\n\n# يمكنك الآن تحليل البيانات بسرعة\nprint(f\"متوسط السعر من ملف Parquet: {df_from_parquet['price'].mean():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T19:37:14.496313Z","iopub.execute_input":"2025-10-16T19:37:14.496666Z","iopub.status.idle":"2025-10-16T19:41:25.581981Z","shell.execute_reply.started":"2025-10-16T19:37:14.496644Z","shell.execute_reply":"2025-10-16T19:41:25.580903Z"}},"outputs":[{"name":"stdout","text":"\nبدء تحويل الملف /kaggle/working/2019-Nov.csv إلى صيغة Parquet...\nاكتمل التحويل إلى Parquet في 209.52 ثانية.\n\nالآن، قراءة ملف Parquet المحول...\nتمت قراءة ملف Parquet بالكامل في 41.24 ثانية فقط!\nمتوسط السعر من ملف Parquet: 292.46\n","output_type":"stream"}],"execution_count":12}]}