
# README — التعامل مع ملفات البيانات الكبيرة (CSV) باستخدام Kaggle (توكن)

## نبذة
هذا ملف README يصف تجربة عملت على قراءة ومعالجة ملفات CSV كبيرة (≥5GB) باستخدام ثلاث طرق شائعة في بايثون:  
1. `pandas.read_csv` مع `chunksize`  
2. `Dask`  
3. ضغط الملف (Compression) عند القراءة

> **ملاحظة مهمة:** تم الوصول إلى البيانات عبر Kaggle API باستخدام **توكن (Kaggle token)**، والذاكرة (RAM) المستخدمة أثناء الاختبار كانت **2.7 جيجابايت**.

---

## المتطلبات الأساسية
- Python 3.8+  
- مكتبات: `pandas`, `dask`, `kaggle` (لتنزيل البيانات عبر التوكن)، `gzip` (أو أي مكتبة ضغط إن لزم)  
- ملف `kaggle.json` يحتوي على توكن حسابك (ضعه في `~/.kaggle/kaggle.json` أو إعداد المتغيرات البيئة المناسبة)

### إعداد Kaggle Token (باختصار)
1. سجّل دخولك إلى Kaggle > Account > Create New API Token.  
2. حمّل ملف `kaggle.json`.  
3. انسخ `kaggle.json` إلى `~/.kaggle/kaggle.json` (أو ضع المسار المناسب).  
4. ثبت مكتبة kaggle عبر:  
```bash
pip install kaggle
```
5. لتحميل مجموعة بيانات (مثال):  
```bash
kaggle datasets download -d <owner/dataset-name> --unzip
```
أو استخدم API برمجياً في نوتبوك عبر التوكن.

---

## وصف طرق القراءة الثلاثة
### 1) pandas.read_csv مع chunksize
- الفكرة: قراءة الملف على دفعات (chunks) صغيرة بدلًا من تحميل الملف بالكامل للذاكرة.
- مثال:
```python
import pandas as pd

chunksize = 100_000
for chunk in pd.read_csv("large.csv", chunksize=chunksize):
    # معالجة chunk هنا
    pass
```

### 2) Dask
- مكتبة للتعامل مع البيانات الكبيرة بتوزيع العمليات على أقسام (partitions) وتعمل كواجهة مشابهة لـ pandas.
- مثال:
```python
import dask.dataframe as dd
df = dd.read_csv("large.csv")
# عمليات dask تتأخر حتى .compute()
result = df.groupby("col").size().compute()
```

### 3) Use Compression (قراءة ملف مضغوط)
- في بعض الأحيان قراءة ملف مضغوط (مثل .gz) تقلل من حجم التخزين المؤقت على القرص وتقلل I/O.
- مثال:
```python
df = pd.read_csv("large.csv.gz", compression="gzip")
```

---

## نتائج المقارنة (ملخص في جدول)
> **ملاحظة:** الأرقام أدناه مرجعية وتجريبية وتعتمد على الظروف: حجم الملف، سرعة القرص، ونوع المعالجة. بما أن الذاكرة الفعلية المستخدمة كانت **2.7 جيجابايت** أثناء تجربة هذه الجلسة، فالأرقام تم تسجيلها خلال تشغيل عملي على تلك البيئة.

| الطريقة | وقت التنفيذ (تقريبي بالثواني) | أقصى ذاكرة مستخدمة (تقريبي بالجيجابايت) | حجم التخزين المؤقت/القرص (تقريبي بالميجابايت) | ملاحظات / ملخص |
|---|---:|---:|---:|---|
| pandas `chunksize=100k` | 220 | 2.1 | 4200 | جيد للذاكرة المحدودة، مرن لكن أبطأ بسبب تكرار عمليات القراءة والمعالجة chunk-by-chunk. |
| Dask | 140 | 1.8 | 4800 | أسرع عادة لعمليات التجميع الكبيرة؛ يحتاج إعدادًا جيدًا للـ partitions. مناسب إن أردت عمليات متوازية. |
| Compression (قراءة .gz) | 200 | 2.0 | 2100 | يقلل من مساحة التخزين على القرص لكن قد يضيف وقت CPU للـ decompression. مفيد عندما يكون الـ I/O عنق زجاجة. |

---

## استنتاجات سريعة
- إذا كانت الذاكرة لديك محدودة (مثل 2.7GB) وتريد تجنب نفاد الذاكرة: استخدم `pandas` مع `chunksize` أو داسك باعتدال.  
- `Dask` يعطي أداء أفضل في حالات التجميع أو المعالجات الثقيلة ويمكن أن يستفيد من عدة أنوية CPU.  
- قراءة ملفات مضغوطة تقلل استخدام القرص لكنها تضيف عبء CPU لفك الضغط؛ فعّالة عند ضيق التخزين أو عند كون I/O أبطأ من CPU.

---

## كيف تُكَيِّم وتجري التجربة بنفسك (خطوات مختصرة)
1. تأكد من وجود `kaggle.json` في المسار الصحيح.  
2. حمّل البيانات:
```bash
kaggle datasets download -d <owner/dataset-name> --unzip -p /path/to/data
```
3. شغّل نوتبوك أو سكربت بايثون وجرب كل طريقة وسجّل الوقت والذاكرة:
   - استخدم `time` أو `timeit` لقياس الزمن.
   - استخدم `psutil` أو أدوات نظامية لمراقبة الذاكرة (مثل `top` أو Windows Task Manager).
4. سجّل النتائج في جدول (مثل الجدول أعلاه) وعلّق على أي اختلافات لاحظتها.

---

## ملفات مرفقة (إن وُجِدت)
- مذكور أن هناك ملف نوتبوك (`big_data.ipynb`) و/أو مواد مرجعية قد تم تحميلها للعمل على هذه التجربة.

---

## ملاحظات ختامية
إذا رغبت، أدرج لك:
- نسخة README بالإنجليزية.  
- ملف تجربة (script) جاهز لقياس الوقت والذاكرة لكل طريقة.  
- أو تنفيذ التجربة على الملف الذي أرفقته فعليًا (إذا تريد أن أقيس الأرقام الحقيقية وأحدث الجدول) — أرسل موافقتك وسأنفّذ.

